s3_id:  <%- ("<"+"%"+"="+" ENV['AWS_ACCESS_KEY_ID'] "+"%"+">") %>
s3_secret: <%- ("<"+"%"+"="+" ENV['AWS_SECRET_ACCESS_KEY'] "+"%"+">") %>
s3_bucket: <%- ("<"+"%"+"="+" ENV['S3_BUCKET_NAME'] "+"%"+">") %>
cloudfront_distribution_id: <%- ("<"+"%"+"="+" ENV['CLOUDFRONT_DISTRIBUTION_ID'] "+"%"+">") %>
s3_endpoint: ap-southeast-2

site: ./dist

index_document: index.html
error_document: index.html # doing this isn't good - but until s3_website is updated to a newer version of the cloudfront api there isn't really an alternative

cache_control:
  "*.html": public, max-age=600 # short cache time for html pages since their filenames don't change
  "*": public, max-age=31536000 # super long cache times because filenames are hashed

gzip:
  - .html
  - .css
  - .js
  - .csv
  - .json
  - .svg
  - .md
  - .eot
  - .ttf
  - .otf


# Don't delete old files. This prevents a situation where a user is looking at a cached version
# of a html page but the css and js resources that the page is requesting has been deleted.
# This will add a bit of bloat to the buckets but the files are small enough that it shouldn't 
# really matter
ignore_on_server: _DELETE_NOTHING_ON_THE_S3_BUCKET_ 


cloudfront_wildcard_invalidation: true
cloudfront_invalidate_root: true

#
#  Unused settings. see https://github.com/laurilehmijoki/s3_website for more
#

# exclude_from_upload:
#   - those_folders_of_stuff
#   - i_wouldnt_want_to_upload

# s3_reduced_redundancy: true

# cloudfront_distribution_id: your-dist-id

# cloudfront_distribution_config:
#   default_cache_behavior:
#     min_TTL: <%= 60 * 60 * 24 %>
#   aliases:
#     quantity: 1
#     items:
#       CNAME: your.website.com


# concurrency_level: 5

# redirects:
#   index.php: /
#   about.php: about.html
#   music-files/promo.mp4: http://www.youtube.com/watch?v=dQw4w9WgXcQ

# routing_rules:
#   - condition:
#       key_prefix_equals: blog/some_path
#     redirect:
#       host_name: blog.example.com
#       replace_key_prefix_with: some_new_path/
#       http_redirect_code: 301
